"""
Test script to demonstrate TruLens RAG evaluation.

Usage:
    python examples/test_evaluation.py
"""
import requests
import json
from datetime import datetime


API_BASE_URL = "http://localhost:8000"


def test_evaluation_endpoint():
    """Test the /evaluate/rag endpoint with sample data."""
    print("\n" + "="*70)
    print("ğŸ§ª Testing RAG Evaluation Endpoint")
    print("="*70)
    
    # Sample evaluation request
    evaluation_data = {
        "question": "What is machine learning?",
        "answer": "Machine learning is a subset of artificial intelligence that enables computers to learn from data and improve their performance over time without being explicitly programmed. It uses algorithms to identify patterns in data and make predictions or decisions.",
        "context": [
            "Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn.",
            "The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future.",
            "Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so."
        ]
    }
    
    print("\nğŸ“ Evaluation Request:")
    print(f"Question: {evaluation_data['question']}")
    print(f"Answer: {evaluation_data['answer'][:100]}...")
    print(f"Context chunks: {len(evaluation_data['context'])}")
    
    # Send request
    print("\nâ³ Sending evaluation request...")
    response = requests.post(
        f"{API_BASE_URL}/evaluate/rag",
        json=evaluation_data,
        headers={"Content-Type": "application/json"}
    )
    
    if response.status_code == 200:
        result = response.json()
        
        print("\nâœ… Evaluation Complete!")
        print("\nğŸ“Š Metrics:")
        metrics = result['metrics']
        print(f"  â€¢ Answer Relevance:  {metrics['answer_relevance']:.3f}")
        print(f"  â€¢ Context Relevance: {metrics['context_relevance']:.3f}")
        print(f"  â€¢ Groundedness:      {metrics['groundedness']:.3f}")
        print(f"  â€¢ Overall Quality:   {metrics['overall_quality']:.3f}")
        
        print("\nğŸ“š Context Stats:")
        stats = result['context_stats']
        print(f"  â€¢ Number of chunks:  {stats['num_chunks']}")
        print(f"  â€¢ Total characters:  {stats['total_chars']}")
        print(f"  â€¢ Avg chunk length:  {stats['avg_chunk_length']}")
        
        print(f"\nâ±ï¸  Evaluation Time: {result['evaluation_time']}s")
        
        # Quality interpretation
        overall = metrics['overall_quality']
        if overall >= 0.8:
            print("\nâœ¨ Quality: Excellent!")
        elif overall >= 0.6:
            print("\nâœ”ï¸  Quality: Good")
        elif overall >= 0.4:
            print("\nâš ï¸  Quality: Fair")
        else:
            print("\nâŒ Quality: Needs improvement")
        
        return result
    else:
        print(f"\nâŒ Error: {response.status_code}")
        print(response.text)
        return None


def test_query_and_evaluate():
    """Test the /evaluate/rag-from-query endpoint (query + evaluate in one step)."""
    print("\n" + "="*70)
    print("ğŸ§ª Testing Query + Evaluation (One Step)")
    print("="*70)
    
    # This requires documents to be indexed first
    question = "What is machine learning?"
    
    print(f"\nğŸ“ Question: {question}")
    print("â³ Processing and evaluating...")
    
    response = requests.post(
        f"{API_BASE_URL}/evaluate/rag-from-query",
        data={
            "question": question,
            "use_llamaindex": True
        }
    )
    
    if response.status_code == 200:
        result = response.json()
        
        print("\nâœ… Complete!")
        print("\nğŸ“– RAG Response:")
        rag_resp = result['rag_response']
        print(f"Answer: {rag_resp['answer'][:200]}...")
        print(f"Sources: {len(rag_resp['sources'])} chunks")
        
        print("\nğŸ“Š Evaluation:")
        eval_data = result['evaluation']
        if 'metrics' in eval_data:
            metrics = eval_data['metrics']
            print(f"  â€¢ Answer Relevance:  {metrics['answer_relevance']:.3f}")
            print(f"  â€¢ Context Relevance: {metrics['context_relevance']:.3f}")
            print(f"  â€¢ Groundedness:      {metrics['groundedness']:.3f}")
            print(f"  â€¢ Overall Quality:   {metrics['overall_quality']:.3f}")
        
        print("\n" + result['summary'])
        
        return result
    else:
        print(f"\nâŒ Error: {response.status_code}")
        if response.status_code == 500:
            print("Note: Make sure documents are uploaded first!")
        print(response.text)
        return None


def main():
    """Run all tests."""
    print("\n" + "="*70)
    print("ğŸš€ TruLens RAG Evaluation Demo")
    print("="*70)
    
    # Test 1: Direct evaluation endpoint
    print("\n\nğŸ“ Test 1: Direct Evaluation")
    print("-" * 70)
    test_evaluation_endpoint()
    
    # Test 2: Query + evaluate (requires documents)
    print("\n\nğŸ“ Test 2: Query + Evaluate (One Step)")
    print("-" * 70)
    print("âš ï¸  Note: This requires documents to be indexed.")
    response = input("\nDo you have documents indexed? (y/n): ")
    
    if response.lower() == 'y':
        test_query_and_evaluate()
    else:
        print("\nğŸ’¡ Tip: Upload some documents first using:")
        print("   POST /documents/upload-file")
        print("   Then try: POST /evaluate/rag-from-query")
    
    print("\n" + "="*70)
    print("âœ… Demo Complete!")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()

