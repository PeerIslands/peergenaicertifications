"""
Framework Comparison: LlamaIndex vs LangChain

This example demonstrates the differences between using LlamaIndex 
and LangChain for RAG, helping you choose the right framework for your needs.

Usage:
    python examples/compare_frameworks.py
"""
import requests
import time
from typing import Dict, Any

API_BASE_URL = "http://localhost:8000"


def print_header(title):
    """Print a formatted header."""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70 + "\n")


def upload_and_compare(pdf_file: str):
    """Upload the same document with both frameworks and compare."""
    print_header("ğŸ“¤ Document Upload Comparison")
    
    results = {}
    
    # Upload with LlamaIndex
    print("ğŸ“˜ Uploading with LlamaIndex (Sentence Window Retrieval)...")
    start_time = time.time()
    try:
        with open(pdf_file, 'rb') as f:
            response = requests.post(
                f"{API_BASE_URL}/documents/upload-file",
                files={"file": f},
                params={"use_llamaindex": True}
            )
        llamaindex_time = time.time() - start_time
        
        if response.status_code == 200:
            print(f"   âœ… Success! Time: {llamaindex_time:.2f}s")
            results['llamaindex_upload'] = response.json()
            results['llamaindex_upload']['time'] = llamaindex_time
        else:
            print(f"   âŒ Failed: {response.text}")
            
    except FileNotFoundError:
        print(f"   âŒ File not found: {pdf_file}")
        return None
    
    # Upload with LangChain
    print("\nğŸ“— Uploading with LangChain (RecursiveCharacterTextSplitter)...")
    start_time = time.time()
    try:
        with open(pdf_file, 'rb') as f:
            response = requests.post(
                f"{API_BASE_URL}/documents/upload-file",
                files={"file": f},
                params={"use_llamaindex": False}
            )
        langchain_time = time.time() - start_time
        
        if response.status_code == 200:
            print(f"   âœ… Success! Time: {langchain_time:.2f}s")
            results['langchain_upload'] = response.json()
            results['langchain_upload']['time'] = langchain_time
        else:
            print(f"   âŒ Failed: {response.text}")
            
    except Exception as e:
        print(f"   âŒ Error: {e}")
    
    # Comparison
    print("\nğŸ” Upload Comparison:")
    print(f"   LlamaIndex: {llamaindex_time:.2f}s")
    print(f"   LangChain:  {langchain_time:.2f}s")
    
    if llamaindex_time < langchain_time:
        print(f"   â†’ LlamaIndex was {(langchain_time/llamaindex_time - 1)*100:.1f}% faster")
    else:
        print(f"   â†’ LangChain was {(llamaindex_time/langchain_time - 1)*100:.1f}% faster")
    
    return results


def query_and_compare(question: str):
    """Ask the same question with both frameworks and compare."""
    print_header(f"â“ Query Comparison: '{question}'")
    
    results = {}
    
    # Query with LlamaIndex
    print("ğŸ“˜ Querying with LlamaIndex...")
    response = requests.post(
        f"{API_BASE_URL}/questions/ask",
        json={
            "question": question,
            "use_llamaindex": True,
            "k": 5
        }
    )
    
    if response.status_code == 200:
        result = response.json()
        results['llamaindex'] = result
        print(f"   âœ… Answer received")
        print(f"   ğŸ“Š Sources: {len(result.get('sources', []))} chunks")
        print(f"   â±ï¸  Time: {result.get('processing_time', 0):.2f}s")
        print(f"   ğŸ“ Answer preview: {result['answer'][:100]}...")
    else:
        print(f"   âŒ Failed: {response.text}")
    
    # Query with LangChain
    print("\nğŸ“— Querying with LangChain...")
    response = requests.post(
        f"{API_BASE_URL}/questions/ask",
        json={
            "question": question,
            "use_llamaindex": False,
            "k": 5
        }
    )
    
    if response.status_code == 200:
        result = response.json()
        results['langchain'] = result
        print(f"   âœ… Answer received")
        print(f"   ğŸ“Š Sources: {len(result.get('sources', []))} chunks")
        print(f"   â±ï¸  Time: {result.get('processing_time', 0):.2f}s")
        print(f"   ğŸ“ Answer preview: {result['answer'][:100]}...")
    else:
        print(f"   âŒ Failed: {response.text}")
    
    # Comparison
    if 'llamaindex' in results and 'langchain' in results:
        print("\nğŸ” Detailed Comparison:")
        
        # Processing time
        llama_time = results['llamaindex'].get('processing_time', 0)
        lang_time = results['langchain'].get('processing_time', 0)
        print(f"\n   â±ï¸  Processing Time:")
        print(f"      LlamaIndex: {llama_time:.2f}s")
        print(f"      LangChain:  {lang_time:.2f}s")
        
        # Source count
        llama_sources = len(results['llamaindex'].get('sources', []))
        lang_sources = len(results['langchain'].get('sources', []))
        print(f"\n   ğŸ“š Sources Retrieved:")
        print(f"      LlamaIndex: {llama_sources} chunks")
        print(f"      LangChain:  {lang_sources} chunks")
        
        # Answer length
        llama_answer_len = len(results['llamaindex']['answer'])
        lang_answer_len = len(results['langchain']['answer'])
        print(f"\n   ğŸ“ Answer Length:")
        print(f"      LlamaIndex: {llama_answer_len} characters")
        print(f"      LangChain:  {lang_answer_len} characters")
        
        # Show full answers
        print("\n   ğŸ“– Full Answers:")
        print(f"\n   LlamaIndex:")
        print(f"      {results['llamaindex']['answer']}")
        print(f"\n   LangChain:")
        print(f"      {results['langchain']['answer']}")
    
    return results


def evaluate_and_compare(question: str):
    """Evaluate responses from both frameworks."""
    print_header("ğŸ“Š Quality Evaluation Comparison")
    
    results = {}
    
    # Evaluate LlamaIndex response
    print("ğŸ“˜ Evaluating LlamaIndex response...")
    response = requests.post(
        f"{API_BASE_URL}/evaluate/rag-from-query",
        data={
            "question": question,
            "use_llamaindex": True
        }
    )
    
    if response.status_code == 200:
        result = response.json()
        if 'metrics' in result['evaluation']:
            results['llamaindex'] = result['evaluation']['metrics']
            metrics = results['llamaindex']
            print(f"   âœ… Evaluation complete")
            print(f"   â€¢ Answer Relevance: {metrics['answer_relevance']:.3f}")
            print(f"   â€¢ Context Relevance: {metrics['context_relevance']:.3f}")
            print(f"   â€¢ Groundedness: {metrics['groundedness']:.3f}")
            print(f"   â€¢ Overall Quality: {metrics['overall_quality']:.3f}")
    
    # Evaluate LangChain response
    print("\nğŸ“— Evaluating LangChain response...")
    response = requests.post(
        f"{API_BASE_URL}/evaluate/rag-from-query",
        data={
            "question": question,
            "use_llamaindex": False
        }
    )
    
    if response.status_code == 200:
        result = response.json()
        if 'metrics' in result['evaluation']:
            results['langchain'] = result['evaluation']['metrics']
            metrics = results['langchain']
            print(f"   âœ… Evaluation complete")
            print(f"   â€¢ Answer Relevance: {metrics['answer_relevance']:.3f}")
            print(f"   â€¢ Context Relevance: {metrics['context_relevance']:.3f}")
            print(f"   â€¢ Groundedness: {metrics['groundedness']:.3f}")
            print(f"   â€¢ Overall Quality: {metrics['overall_quality']:.3f}")
    
    # Comparison
    if 'llamaindex' in results and 'langchain' in results:
        print("\nğŸ† Winner by Metric:")
        
        llama_metrics = results['llamaindex']
        lang_metrics = results['langchain']
        
        metrics_to_compare = [
            ('answer_relevance', 'Answer Relevance'),
            ('context_relevance', 'Context Relevance'),
            ('groundedness', 'Groundedness'),
            ('overall_quality', 'Overall Quality')
        ]
        
        for metric_key, metric_name in metrics_to_compare:
            llama_score = llama_metrics[metric_key]
            lang_score = lang_metrics[metric_key]
            
            if llama_score > lang_score:
                winner = "ğŸ“˜ LlamaIndex"
                diff = ((llama_score - lang_score) / lang_score * 100) if lang_score > 0 else 0
            elif lang_score > llama_score:
                winner = "ğŸ“— LangChain"
                diff = ((lang_score - llama_score) / llama_score * 100) if llama_score > 0 else 0
            else:
                winner = "ğŸ¤ Tie"
                diff = 0
            
            print(f"   {metric_name:20s}: {winner:20s} (+{diff:.1f}%)")
    
    return results


def show_recommendations():
    """Show framework recommendations."""
    print_header("ğŸ’¡ Framework Recommendations")
    
    print("ğŸ“˜ Use LlamaIndex when:")
    print("   âœ… Answer quality is your top priority")
    print("   âœ… You need advanced retrieval (Sentence Window)")
    print("   âœ… You're building a production RAG system")
    print("   âœ… You need better metadata handling")
    print("   âœ… You want the latest RAG innovations")
    
    print("\nğŸ“— Use LangChain when:")
    print("   âœ… You need conversation memory")
    print("   âœ… You're building complex agent workflows")
    print("   âœ… You need broader LLM ecosystem integration")
    print("   âœ… You're familiar with LangChain already")
    print("   âœ… You need more flexibility for custom chains")
    
    print("\nğŸ¯ Default Recommendation:")
    print("   â†’ Use LlamaIndex for RAG applications")
    print("   â†’ It's optimized specifically for retrieval tasks")
    print("   â†’ Sentence Window Retrieval provides better context")


def main():
    """Run framework comparison."""
    print("\n" + "="*70)
    print("  ğŸ”„ Framework Comparison: LlamaIndex vs LangChain")
    print("="*70)
    
    # Check service
    try:
        response = requests.get(f"{API_BASE_URL}/health")
        if response.status_code != 200:
            print("\nâŒ Service is not running. Start it with: python main.py")
            return
    except Exception as e:
        print(f"\nâŒ Cannot connect to service: {e}")
        return
    
    # Get PDF file
    print("\nğŸ“„ To compare frameworks, we need a PDF document.")
    pdf_file = input("Enter PDF file path: ").strip()
    
    if not pdf_file:
        print("âŒ No file provided. Exiting.")
        return
    
    # Run comparisons
    print("\nStarting comprehensive comparison...")
    print("(This will take a few minutes...)\n")
    
    # 1. Upload comparison
    upload_results = upload_and_compare(pdf_file)
    
    if not upload_results:
        print("âŒ Upload failed. Cannot continue.")
        return
    
    # 2. Query comparison
    test_questions = [
        "What is the main topic of this document?",
        "Can you summarize the key points?",
        "What are the most important details?"
    ]
    
    for question in test_questions:
        query_and_compare(question)
    
    # 3. Quality evaluation
    evaluate_and_compare(test_questions[0])
    
    # 4. Recommendations
    show_recommendations()
    
    # Summary
    print("\n" + "="*70)
    print("  âœ… Comparison Complete!")
    print("="*70)
    print("\nğŸ“ Key Takeaways:")
    print("   â€¢ Both frameworks are powerful for RAG")
    print("   â€¢ LlamaIndex excels at retrieval quality")
    print("   â€¢ LangChain excels at complex workflows")
    print("   â€¢ Choose based on your specific needs")
    print("\nğŸ”— Learn more:")
    print("   â€¢ LlamaIndex: https://docs.llamaindex.ai/")
    print("   â€¢ LangChain: https://python.langchain.com/")
    print("\n")


if __name__ == "__main__":
    main()

