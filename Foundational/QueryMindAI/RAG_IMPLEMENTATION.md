# RAG (Retrieval-Augmented Generation) Implementation

This implementation adds RAG functionality to QueryMind AI using your existing MongoDB setup with embeddings generated by the 'embeddinggemma' model.

## What's Been Implemented

### Core Components

1. **MongoDB Service** (`server/mongodb.ts`)
   - Connects to your MongoDB Atlas database
   - Implements cosine similarity search using aggregation pipeline
   - Searches documents with embeddings in the 'embedding' field

2. **Embeddings Service** (`server/embeddings.ts`)
   - Uses the 'embeddinggemma' model for generating query embeddings
   - Compatible with your existing embedding generation setup

3. **RAG Service** (`server/rag.ts`)
   - Combines retrieval and generation
   - Retrieves relevant documents based on query similarity
   - Formats context for the LLM

4. **Updated LangChain Service** (`server/langchain.ts`)
   - Integrated RAG into the chat system
   - Automatically retrieves relevant context for each message
   - Includes sources in responses

5. **Updated Routes** (`server/routes.ts`)
   - Added RAG endpoints for searching and status checking
   - MongoDB connection initialization
   - Enhanced health check with RAG status

## Configuration

The system uses these environment variables (already configured):

```env
MONGODB_URI=mongodb+srv://mongosh_aj:ajinkya123@cluster0.clx9fur.mongodb.net/
MONGODB_DATABASE=query-mind
MONGODB_COLLECTION=knowledge-base
OLLAMA_BASE_URL=http://localhost:11434
```

## How It Works

1. **User sends a message** → Chat API endpoint
2. **Generate query embedding** → Using 'embeddinggemma' model
3. **Search MongoDB** → Find similar documents using cosine similarity
4. **Retrieve context** → Get relevant content from knowledge base
5. **Generate response** → LLM uses context + conversation history
6. **Return response** → Include sources for transparency

## API Endpoints

### Chat with RAG
```
POST /api/chat
{
  "message": "Tell me about AI",
  "sessionId": "session-123"
}
```

Response includes:
- AI response enhanced with retrieved context
- Sources used (if any)

### Search Knowledge Base
```
GET /api/rag/search?query=artificial intelligence&limit=5
```

### Check RAG Status
```
GET /api/rag/status
```

### Health Check
```
GET /api/health
```
Now includes RAG status and document count.

## Testing

Run the test script to verify RAG functionality:

```bash
npx tsx server/test-rag.ts
```

## Prerequisites

1. **Ollama running** with this model:
   - `embeddinggemma` (for embeddings)

2. **MongoDB Atlas** with documents containing:
   - `content` field with text
   - `embedding` field with vector embeddings
   - Optional `metadata` field

## Usage

1. **Start the server**: `npm run dev`
2. **Send chat messages** - RAG is automatically enabled
3. **Check status**: Visit `/api/health` or `/api/rag/status`
4. **Search directly**: Use `/api/rag/search` endpoint

## Features

- ✅ Automatic context retrieval for each chat message
- ✅ Cosine similarity search in MongoDB
- ✅ Source attribution in responses
- ✅ Graceful fallback if RAG fails
- ✅ Status monitoring and health checks
- ✅ Compatible with existing embedding setup

The RAG system is now fully integrated and will enhance your chat responses with relevant context from your knowledge base!
