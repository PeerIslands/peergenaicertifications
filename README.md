# RAG Chat App with Pre-loaded PDF Documents

A complete local Retrieval-Augmented Generation (RAG) chat application that automatically processes PDF documents from the files directory and allows querying them in natural language.

## ğŸ—ï¸ Architecture

- **Frontend**: React + TailwindCSS
- **Backend**: FastAPI (Python)
- **Database**: MongoDB with vector indexing
- **Model**: Local LLaMA 3 (via Ollama)
- **Embeddings**: HuggingFace sentence-transformers/all-MiniLM-L6-v2

## ğŸ“ Project Structure

```
rag-chat-app/
â”œâ”€â”€ backend-rag-chat/             # FastAPI Backend
â”‚   â”œâ”€â”€ main.py                   # FastAPI application
â”‚   â”œâ”€â”€ config.py                 # Configuration settings
â”‚   â”œâ”€â”€ routes/                   # API routes
â”‚   â”‚   â”œâ”€â”€ documents.py          # Document management endpoints
â”‚   â”‚   â””â”€â”€ chat.py               # Chat endpoints
â”‚   â”œâ”€â”€ services/                 # Business logic
â”‚   â”‚   â”œâ”€â”€ file_processor.py     # File processing from directory
â”‚   â”‚   â”œâ”€â”€ embeddings.py         # Text embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # MongoDB vector operations
â”‚   â”‚   â””â”€â”€ rag_pipeline.py       # RAG pipeline
â”‚   â”œâ”€â”€ models/                   # Pydantic models
â”‚   â”‚   â”œâ”€â”€ chat_request.py       # Chat request/response models
â”‚   â”‚   â””â”€â”€ pdf_metadata.py       # PDF metadata models
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ env.example               # Environment template
â”œâ”€â”€ frontend-rag-chat/            # React Frontend
â”‚   â”œâ”€â”€ src/                      # React source code
â”‚   â”‚   â”œâ”€â”€ components/           # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatWindow.jsx   # Chat display component
â”‚   â”‚   â”‚   â””â”€â”€ ChatInput.jsx    # Chat input component
â”‚   â”‚   â”œâ”€â”€ services/            # API services
â”‚   â”‚   â”‚   â””â”€â”€ api.js           # Axios API client
â”‚   â”‚   â”œâ”€â”€ App.js               # Main React app
â”‚   â”‚   â””â”€â”€ index.js             # React entry point
â”‚   â”œâ”€â”€ public/                   # Static files
â”‚   â”œâ”€â”€ package.json             # Node.js dependencies
â”‚   â””â”€â”€ tailwind.config.js        # TailwindCSS config
â”œâ”€â”€ run_backend.sh               # Backend startup script
â”œâ”€â”€ run_frontend.sh              # Frontend startup script
â”œâ”€â”€ setup.sh                     # Automated setup script
â”œâ”€â”€ docker-compose.yml           # Docker setup
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Setup Instructions

### Prerequisites

1. **Python 3.8+**
2. **Node.js 16+**
3. **MongoDB** (local installation or MongoDB Atlas)
4. **Ollama** with LLaMA 3 model

### 1. Install Ollama and LLaMA 3

```bash
# Install Ollama (macOS)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull LLaMA 3 model
ollama pull llama3
```

### 2. Setup Backend

```bash
# Navigate to backend directory
cd backend-rag-chat

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
cp env.example .env
# Edit .env with your MongoDB URI and other settings

# Start MongoDB (if running locally)
mongod

# Run the backend
uvicorn main:app --reload
```

### 3. Setup Frontend

```bash
# Navigate to frontend directory
cd frontend-rag-chat

# Install dependencies
npm install

# Start the frontend
npm start
```

### 4. Access the Application

- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Documentation: http://localhost:8000/docs

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the project root:

```env
MONGODB_URI=mongodb://localhost:27017
DATABASE_NAME=rag_chat_db
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
LLAMA_MODEL_ENDPOINT=http://localhost:11434/api/generate
MAX_FILES=10
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_CHUNKS=5
```

### MongoDB Vector Index

The application will automatically create a vector index for similarity search. For production, consider using MongoDB Atlas with vector search capabilities.

## ğŸ“– Usage

1. **Automatic Processing**: PDF files in the `files/` directory are automatically processed on startup
2. **Chat**: Ask questions about the loaded documents
3. **View Sources**: See which documents were used to answer your questions
4. **Conversation Memory**: The system maintains context across multiple questions in a session

## ğŸ”Œ API Endpoints

### Document Management
- `POST /api/reload` - Reload and process all files from files directory
- `GET /api/status` - Get current document status

### Chat
- `POST /api/chat` - Send chat message
- `DELETE /api/reset` - Clear all documents
- `GET /api/health` - Health check

## ğŸ› ï¸ Development

### Backend Development

```bash
# Run with auto-reload
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Development

```bash
# Run development server
npm start

# Build for production
npm run build
```

## ğŸ³ Docker Support (Optional)

Create `Dockerfile` for backend:

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ” Troubleshooting

### Common Issues

1. **MongoDB Connection**: Ensure MongoDB is running and accessible
2. **Ollama Connection**: Verify Ollama is running and LLaMA 3 model is available
3. **CORS Issues**: Check that frontend and backend are on correct ports
4. **Memory Issues**: Large PDFs may require more RAM for processing

### Logs

- Backend logs: Check terminal where uvicorn is running
- Frontend logs: Check browser console
- MongoDB logs: Check MongoDB log files

## ğŸ“ Features

- âœ… Automatic PDF processing from files directory
- âœ… Text extraction and chunking
- âœ… Vector embeddings with HuggingFace
- âœ… MongoDB vector storage
- âœ… RAG pipeline with LLaMA 3
- âœ… Real-time chat interface
- âœ… Source attribution
- âœ… Conversation memory and context
- âœ… Responsive design
- âœ… Error handling