ðŸ“Œ 1. Install Required Software
ðŸ”¹ Install Python 3.10+

Download from:
https://www.python.org/downloads/

Select:
âœ” Add to PATH
âœ” Install pip

ðŸ”¹ Install Ollama (Local LLM Runner)

Download from:
https://ollama.com/download

After install, verify:

ollama --version

ðŸ”¹ Install Any LLM You Want (example: Gemma 3:1B)
ollama pull gemma3:1b


ðŸ“Œ 2. Clone Project & Enter Backend Folder
git clone <your-repo-url>
cd backend

ðŸ“Œ 3. Create & Activate Virtual Environment
Windows
python -m venv venv
venv\Scripts\activate

macOS/Linux
python3 -m venv venv
source venv/bin/activate

ðŸ“Œ 4. Install Backend Dependencies
pip install -r requirements.txt


Make sure your requirements.txt contains:

flask                         # Web framework for building the API backend / routes
pypdf                         # Used to read and extract text from PDF files
langchain                     # Framework to build LLM apps (RAG, chains, prompts)
chromadb                      # Vector database to store embeddings for retrieval
sentence-transformers         # To generate embeddings for text/PDF content
ollama                        # Local LLM runner (Gemma, Llama, etc.)
faiss-cpu  


ðŸ“Œ 5. Run Ingestion for Any PDF
python ingest.py 


This will:

âœ” Load PDF
âœ” Split text
âœ” Store into a Chroma collection
âœ” Ready for querying

ðŸ“Œ 6. Start Backend Server
python app.py


Backend will run at:

http://127.0.0.1:5000/query

ðŸŸ© FRONTEND SETUP (Normal .html file)

This is a simple UI with:

âœ” Ask question
âœ” Display Answer
